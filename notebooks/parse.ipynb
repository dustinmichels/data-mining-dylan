{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_album_urls():\n",
    "    \"\"\"Returns list of URLs to every album\"\"\"\n",
    "    page = requests.get('https://bobdylan.com/albums/')\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return ([\n",
    "        x.a['href']\n",
    "        for x in soup.find_all('h3')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_urls = get_album_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_url = album_urls[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(album_url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://cdn.smehost.net/bobdylancom-uscolumbiaprod/wp-content/uploads/2011/11/5701829_179.jpg'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cover = soup.find(class_='cover').find('img')['src']\n",
    "cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Best Of The Original Mono Recordings', '2010')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline = soup.find(class_='headline').get_text(strip=True)\n",
    "title = headline[:headline.find('(')].strip()\n",
    "year = headline[headline.find(\"(\")+1:headline.find(\")\")]\n",
    "title, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_list = [\n",
    "    x.get_text(strip='true').split('. ')[1]\n",
    "    for x in soup.find_all(class_='track')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_album_data(album_url):\n",
    "    page = requests.get(album_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    cover_url = soup.find(class_='cover').find('img')['src']\n",
    "    headline = (soup.find(class_='headline').get_text(strip=True))\n",
    "    title = headline[:headline.find('(')].strip()\n",
    "    year = headline[headline.find(\"(\")+1:headline.find(\")\")]\n",
    "    \n",
    "    track_list = [\n",
    "        x.get_text(strip='true').split('. ')[1]\n",
    "        for x in  soup.find_all(class_='track')]\n",
    "    \n",
    "    return dict(\n",
    "        title=title, year=year, cover_url=cover_url,\n",
    "        track_list=track_list, url=album_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = get_album_data(album_urls[11])\n",
    "d2 = get_album_data(album_urls[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "albums = [get_album_data(url) for url in album_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/albums.json', 'w') as f:\n",
    "    json.dump(albums, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from my JSON\n",
    "df = pd.read_json('data/albums.json')\n",
    "df.set_index('title', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_setlist_urls(filter_url):\n",
    "    page = requests.get(filter_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    tour_urls = soup.find(class_='tour-list').find_all('a', {'class', 'date'})\n",
    "    return [x['href'] for x in tour_urls]\n",
    "\n",
    "def get_filtered_tour_urls():\n",
    "    page = requests.get('https://www.bobdylan.com/setlists')\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    return [\n",
    "        x['href']\n",
    "        for x in soup.find(class_='buy-options').find_all('a')]\n",
    "\n",
    "def get_all_setlist_urls():\n",
    "    all_setlist_urls = []\n",
    "    filter_urls = get_filtered_tour_urls()\n",
    "    for url in filter_urls:\n",
    "        all_setlist_urls.extend(get_setlist_urls(url))\n",
    "    return all_setlist_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setlist_urls = get_all_setlist_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(setlist_url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = soup.find(class_='date').get_text()\n",
    "place = soup.find(class_='headline').get_text()\n",
    "venue = soup.find(class_='venue').get_text()\n",
    "\n",
    "setlist = [\n",
    "    x.get_text()\n",
    "    for x in soup.find(class_='set-list').find_all('a')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_show_data(setlist_url):\n",
    "    print(setlist_url)\n",
    "    page = requests.get(setlist_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    date = safe_parse(lambda: soup.find(class_='date').get_text())\n",
    "    place = safe_parse(lambda: soup.find(class_='headline').get_text())\n",
    "    venue = safe_parse(lambda: soup.find(class_='venue').get_text())\n",
    "    _setlist = safe_parse(lambda: soup.find(class_='set-list').find_all(class_='title'))\n",
    "    setlist = [x.get_text() for x in _setlist]\n",
    "    \n",
    "    print(dict(date=date, place=place, venue=venue, setlist=setlist))\n",
    "    return dict(date=date, place=place, venue=venue, setlist=setlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_parse(func):\n",
    "    \"\"\"Catches parsing errors\"\"\"\n",
    "    try:\n",
    "        return func()\n",
    "    except AttributeError:\n",
    "        print(\"  Couldn't parse!\")\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/shows.json', 'w') as f:\n",
    "    json.dump(show_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = pd.read_json('data/songs.json').set_index('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "album_df = pd.read_json('data/albums.json').set_index('title')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
